{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-1vOMEXIhMQt"
      },
      "source": [
        "##### Copyright 2020 The TensorFlow Hub Authors.\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "pRfq9ZU5hQhg"
      },
      "outputs": [],
      "source": [
        "#@title Copyright 2020 The TensorFlow Hub Authors. All Rights Reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "# =============================================================================="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mTL0TERThT6z"
      },
      "source": [
        "\u003ctable class=\"tfo-notebook-buttons\" align=\"left\"\u003e\n",
        "  \u003ctd\u003e\n",
        "    \u003ca target=\"_blank\" href=\"https://www.tensorflow.org/hub/tutorials/bert_experts\"\u003e\u003cimg src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" /\u003eView on TensorFlow.org\u003c/a\u003e\n",
        "  \u003c/td\u003e\n",
        "  \u003ctd\u003e\n",
        "    \u003ca target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/hub/blob/master/examples/colab/bert_experts.ipynb\"\u003e\u003cimg src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" /\u003eRun in Google Colab\u003c/a\u003e\n",
        "  \u003c/td\u003e\n",
        "  \u003ctd\u003e\n",
        "    \u003ca target=\"_blank\" href=\"https://github.com/tensorflow/hub/blob/master/examples/colab/bert_experts.ipynb\"\u003e\u003cimg src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" /\u003eView source on GitHub\u003c/a\u003e\n",
        "  \u003c/td\u003e\n",
        "  \u003ctd\u003e\n",
        "    \u003ca href=\"https://storage.googleapis.com/tensorflow_docs/hub/examples/colab/bert_experts.ipynb\"\u003e\u003cimg src=\"https://www.tensorflow.org/images/download_logo_32px.png\" /\u003eDownload notebook\u003c/a\u003e\n",
        "  \u003c/td\u003e\n",
        "\u003c/table\u003e"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FkthMlVk8bHp"
      },
      "source": [
        "# BERT Experts from TF-Hub\n",
        "\n",
        "This colab demonstrates how to:\n",
        "* Load BERT models from [TensorFlow Hub](https://tfhub.dev) that have been trained on different tasks including MNLI, SQuAD, and PubMed\n",
        "* Use a SentencePiece model using the BERT model vocabulary to tokenize raw text and convert it to ids\n",
        "* Generate the pooled and sequence output from the token input ids using the loaded model\n",
        "* Look at the semantic similarity of the pooled outputs of different sentences\n",
        "\n",
        "#### Note: This colab should be run with a TPU runtime"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jspO02jDPfPG"
      },
      "source": [
        "## Set up and imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r-ed8zj-dbwm"
      },
      "outputs": [],
      "source": [
        "!pip3 install --quiet tensorflow\n",
        "!pip3 install --quiet sentencepiece\n",
        "!pip3 install --quiet tf-models-official"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "czDmtrGKYw_5"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import pairwise\n",
        "\n",
        "import tensorflow.compat.v2 as tf\n",
        "import tensorflow_hub as hub\n",
        "import sentencepiece as spm\n",
        "from official.nlp.bert import tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dIgyPyFBcgpK"
      },
      "source": [
        "## TPU Configuration\n",
        "\n",
        "Let's configure the TPUs to be able to run the BERT model quickly.\n",
        "\n",
        "Go to **Runtime** → **Change runtime type** to make sure that **TPU** is selected"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mA-ftP4WcgTs"
      },
      "outputs": [],
      "source": [
        "resolver = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "tf.config.experimental_connect_to_cluster(resolver)\n",
        "tf.tpu.experimental.initialize_tpu_system(resolver)\n",
        "print(\"All devices: \", tf.config.list_logical_devices('TPU'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "GXrSO2Vc1Qtr"
      },
      "outputs": [],
      "source": [
        "#@title Helper functions for tokenization\n",
        "\n",
        "def build_input(tokenizer, sentence, max_seq_length):\n",
        "  \"\"\"Generate (input_ids, input_mask, segment_ids) for a single sentence.\"\"\"\n",
        "  tokens = tokenizer.tokenize(sentence)\n",
        "  tokens = [\"[CLS]\"] + tokens + [\"[SEP]\"]\n",
        "  ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "  # Pad the ids to max sequence length\n",
        "  pad_len = max_seq_length - len(ids)\n",
        "  input_ids = ids + [0]*pad_len\n",
        "  input_mask = [1]*len(ids) + [0]*pad_len\n",
        "\n",
        "  # Single sentence segment_ids are all 0\n",
        "  segment_ids = [0]*max_seq_length\n",
        "  return (input_ids, input_mask, segment_ids)\n",
        "\n",
        "\n",
        "def build_inputs(tokenizer, sentences, max_seq_length):\n",
        "  \"\"\"Generate (input_ids, input_mask, segment_ids) for a batch of sentences.\"\"\"\n",
        "  inputs = [build_input(tokenizer, s, max_seq_length) for s in sentences]\n",
        "\n",
        "  # Slice to batch each input tensor\n",
        "  input_ids = np.array([x[0] for x in inputs], dtype=np.int32)\n",
        "  input_masks = np.array([x[1] for x in inputs], dtype=np.int32)\n",
        "  segment_ids = np.array([x[2] for x in inputs], dtype=np.int32)\n",
        "  return [input_ids, input_masks, segment_ids]\n",
        "\n",
        "\n",
        "def reconstruct_tokens(tokenizer, ids):\n",
        "  \"\"\"Map the input_ids from a batch of bert ids to tokens.\"\"\"\n",
        "  batched_input_ids = ids[0]\n",
        "  tokens = [tokenizer.convert_ids_to_tokens(input_ids) for input_ids in batched_input_ids]\n",
        "  return tokens\n",
        "\n",
        "\n",
        "def plot_similarity(features, labels):\n",
        "  \"\"\"Plot a similarity matrix of the embeddings.\"\"\"\n",
        "  cos_sim = pairwise.cosine_similarity(features)\n",
        "  sns.set(font_scale=1.2)\n",
        "  cbar_kws=dict(use_gridspec=False, location=\"left\")\n",
        "  g = sns.heatmap(\n",
        "      cos_sim, xticklabels=labels, yticklabels=labels,\n",
        "      vmin=0, vmax=1, cmap=\"Blues\", cbar_kws=cbar_kws)\n",
        "  g.tick_params(labelright=True, labelleft=False)\n",
        "  g.set_yticklabels(labels, rotation=0)\n",
        "  g.set_title(\"Semantic Textual Similarity\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "both",
        "id": "GSuDcPSaY5aB"
      },
      "outputs": [],
      "source": [
        "#@title Configure the model { run: \"auto\" }\n",
        "BERT_MODEL = \"https://tfhub.dev/google/experts/bert/wiki_books/1\" # @param {type: \"string\"} [\"https://tfhub.dev/google/experts/bert/wiki_books/1\", \"https://tfhub.dev/google/experts/bert/wiki_books/mnli/1\", \"https://tfhub.dev/google/experts/bert/wiki_books/qnli/1\", \"https://tfhub.dev/google/experts/bert/wiki_books/qqp/1\", \"https://tfhub.dev/google/experts/bert/wiki_books/squad2/1\", \"https://tfhub.dev/google/experts/bert/wiki_books/sst2/1\",  \"https://tfhub.dev/google/experts/bert/pubmed/1\", \"https://tfhub.dev/google/experts/bert/pubmed/squad2/1\"]\n",
        "MAX_SEQUENCE_LENGTH =  512 #@param {type: \"integer\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pvaZiGVgwtqw"
      },
      "source": [
        "## Sentences\n",
        "\n",
        "Let's take some sentences from Wikipedia to run through model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tytu-rSpeDNG"
      },
      "outputs": [],
      "source": [
        "sentences = [\n",
        "  \"Here We Go Then, You And I is a 1999 album by Norwegian pop artist Morten Abel. It was Abel's second CD as a solo artist.\",\n",
        "  \"The album went straight to number one on the Norwegian album chart, and sold to double platinum.\",\n",
        "  \"Among the singles released from the album were the songs \\\"Be My Lover\\\" and \\\"Hard To Stay Awake\\\".\",\n",
        "  \"Riccardo Zegna is an Italian jazz musician.\",\n",
        "  \"Rajko Maksimović is a composer, writer, and music pedagogue.\",\n",
        "  \"One of the most significant Serbian composers of our time, Maksimović has been and remains active in creating works for different ensembles.\",\n",
        "  \"Ceylon spinach is a common name for several plants and may refer to: Basella alba Talinum fruticosum\",\n",
        "  \"A solar eclipse occurs when the Moon passes between Earth and the Sun, thereby totally or partly obscuring the image of the Sun for a viewer on Earth.\",\n",
        "  \"A partial solar eclipse occurs in the polar regions of the Earth when the center of the Moon's shadow misses the Earth.\",\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zI39475kxCKh"
      },
      "source": [
        "## Run the model\n",
        "\n",
        "We'll load the model from TF-Hub, tokenize our sentences using a SentencePiece tokenizer built from our model vocab, then feed in the tokenized sentences to the model. To keep this colab fast and simple, we'll use a single TPU core to compute the BERT embeddings. To learn how to use all the TPU devices available, see the TensorFlow [TPU](https://www.tensorflow.org/guide/tpu) and [Distributed Training](https://www.tensorflow.org/guide/distributed_training) guides."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x4t6r22ErQg0"
      },
      "outputs": [],
      "source": [
        "with tf.device(\"/device:TPU:0\"):\n",
        "  bert = hub.load(BERT_MODEL)\n",
        "  vocab_path = bert.vocab_file.asset_path.numpy()\n",
        "  tokenizer = tokenization.FullTokenizer(vocab_path, do_lower_case=bert.do_lower_case)\n",
        "  inputs = build_inputs(tokenizer, sentences, MAX_SEQUENCE_LENGTH)\n",
        "  pooled_output, sequence_output = bert(inputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gItjCg4315Cv"
      },
      "outputs": [],
      "source": [
        "print(\"Sentences:\")\n",
        "print(sentences)\n",
        "\n",
        "print(\"\\nTokenized sentences:\")\n",
        "print([tokenizer.tokenize(s) for s in sentences])\n",
        "\n",
        "print(\"\\nBERT inputs:\")\n",
        "print(inputs)\n",
        "\n",
        "print(\"\\nPooled embeddings:\")\n",
        "print(pooled_output)\n",
        "\n",
        "print(\"\\nPer token embeddings:\")\n",
        "print(sequence_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ptiW2mgw6x-l"
      },
      "source": [
        "## Semantic similarity\n",
        "\n",
        "Now let's take a look at the `pooled_output` embeddings of our sentences and compare how similar they are across sentences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "td6jcT0pJMZ5"
      },
      "outputs": [],
      "source": [
        "plot_similarity(pooled_output, sentences)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJ4QCyzhSL7B"
      },
      "source": [
        "## Learn more\n",
        "\n",
        "* Find more BERT models on [TensorFlow Hub](https://tfhub.dev)\n",
        "* This notebook demonstrates simple inference with BERT, you can find a more advanced tutorial about fine-tuning BERT at [tensorflow.org/official_models/fine_tuning_bert](https://www.tensorflow.org/official_models/fine_tuning_bert)\n",
        "* We used just one TPU chip to run the model, you can learn more about how to load models using tf.distribute at [tensorflow.org/tutorials/distribute/save_and_load](https://www.tensorflow.org/tutorials/distribute/save_and_load)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "collapsed_sections": [],
      "name": "bert_experts.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
