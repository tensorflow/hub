# Module microsoft-ai-for-earth/vision/detector/megadetector_V3/1
Object detection model for camera trap images.

<!-- fine-tunable: false -->
<!-- format: hub -->
<!-- asset-path: https://lilablobssc.blob.core.windows.net/models/camera_traps/megadetector/saved_model_normalized_megadetector_v3_tf19.tar.gz -->
<!-- module-type: image-object-detection -->
<!-- interactive-model-name: megadetector_v3 -->
<!-- license: MIT -->

## Overview

This model is licensed under
[MIT](https://github.com/Microsoft/dotnet/blob/master/LICENSE), see below for
details.

### Module description

A detector model to localize animals in camera trap images, built by
 [Microsoft AI for Earth](https://www.microsoft.com/en-us/ai/ai-for-earth).

### Input

Inputs are expected to be 3-channel RGB color images.

### Output

This model outputs to `BoxPredictor_0`.

*   **`BoxPredictor_0`**: Detection boxes and scores.
    *    **num_detections**: Number of detections.
    *    **detection_boxes**: Bounding boxes of each detection.
    *    **detection_classes**: Class ID of each detection.
    *    **detection_scores**: Confidence score of each detection.

## Usage

### Use SavedModel in Python

The model can be loaded in a Python script as follows:

```python
module = hub.Module("https://tfhub.dev/microsoft-ai-for-earth/vision/detector/megadetector_V3/1")
height, width = hub.get_expected_image_size(module)
images = ...  # A batch of images with shape [batch_size, height, width, 3].
output_dict = module(images)  # Output dictionary.
```

The input images are expected to have color values in the range [0,1], following
 the
 [common image input](https://www.tensorflow.org/hub/common_signatures/images#input)
 conventions. 

Fine-tuning is not currently supported.


## Training

### Training dataset

The weights for this module were obtained by training on camera trap datasets,
 some of which are publicly available at [lila.science](http://lila.science/).
 The datasets on lila.science are available for public use under the
 [CDLA](https://cdla.io/permissive-1-0/).

### Model architecture

This model is based on the inception\_resnet\_v2 and faster\_rcnn architectures.

[Inception-ResNet-V2](https://github.com/tensorflow/models/blob/master/research/slim/nets/inception_resnet_v2.py)
 is a neural network architecture for image classification, originally published
 by

    Szegedy, Christian, et al. “Inception-v4, inception-resnet and the impact of
    residual connections on learning.” AAAI. Vol. 4. 2017.

[Faster-RCNN](https://github.com/tensorflow/models/blob/master/research/object_detection/meta_architectures/faster_rcnn_meta_arch.py)
 is a neural network architecture for object detection in images, originally
 published by

    Ren, Shaoqing, et al. "Faster r-cnn: Towards real-time object detection with
    region proposal networks." Advances in neural information processing
    systems. 2015.

### Training procedure

Training was done on Azure GPUs. Model architecture and training procedure
 hyperparameters can be found
 [here](https://lilablobssc.blob.core.windows.net/models/camera_traps/megadetector/megadetector_v3.config).


### Export information
This module was exported based on this
 [checkpoint](https://lilablobssc.blob.core.windows.net/models/camera_traps/megadetector/saved_model_normalized_megadetector_v3_tf19.tar.gz).

### Additional information
Supports interactive model visualization.

## Suitable Use, Limitations, and Fair Use Terms

### Suitable use cases

*   Localize animals in camera trap images, sort out empty images from those
    containing animals or people.

### Limitations

*   This model may not generalize to non camera trap images.
*   If you would like to run this model on large sets of images,
    there are tools for doing so at the [MegaDetector GitHub Page](https://github.com/microsoft/CameraTraps/blob/master/megadetector.md).


### License

This model is released under the
 [MIT](https://github.com/Microsoft/dotnet/blob/master/LICENSE) license. If
 you intend to use it beyond permissible usage, please consult with the model
 owners ahead of time.

## Citation

When used for publication or production, please cite this model as follows:

@article{beery2019efficient, \
 title={Efficient Pipeline for Camera Trap Image Review}, \
 author={Beery, Sara and Morris, Dan and Yang, Siyu}, \
 journal={arXiv preprint arXiv:1907.06772}, \
 year={2019}
}

This Hub module can also be cited as:

Microsoft AI For Earth. (2019). Microsoft AI for Earth Camera Trap MegaDetector
Model V3. Tensorflow Hub. https://doi.org/10.26161/m30x-pa17
