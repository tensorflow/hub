# Module google/imagenet/mobilenet_v2_050_160/feature_vector/1

**Module URL:** https://storage.googleapis.com/tfhub-test-modules/google/imagenet/mobilenet_v2_050_160/feature_vector/1.tar.gz

## Overview

MobileNet V2 is a family of neural network architectures for efficient
on-device image classification and related tasks, originally published by

  * Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov,
    Liang-Chieh Chen: ["Inverted Residuals and Linear Bottlenecks:
    Mobile Networks for Classification, Detection and
    Segmentation"](https://arxiv.org/abs/1801.04381), 2018.

Mobilenets come in various sizes controlled by a multiplier for the
depth (number of features) in the convolutional layers. They can also be
trained for various sizes of input images to control inference speed.

This TF-Hub module uses the TF-Slim implementation of
`mobilenet_v2`
with a depth multiplier of 0.5 and an input size of
160x160 pixels.
This implementation of Mobilenet V2 rounds feature depths to multiples of 8
(an optimization *not* described in the paper).
Depth multipliers less than 1.0 are not applied to the last convolutional layer
(from which the module takes the image feature vector).


The module contains a trained instance of the network, packaged to get
[feature vectors from images](../../../../../common_signatures/images.md#image-feature-vector).
If you want the full model including the classification it was originally
trained for, use module
[`google/imagenet/mobilenet_v2_050_160/classification/1`](../classification/1.md)
instead.


## Training

The checkpoint exported into this module was `mobilenet_v2_0.5_160/mobilenet_v2_0.5_160.ckpt` downloaded
from
[MobileNet V2 pre-trained models](https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/README.md).
Its weights were originally obtained by training on the ILSVRC-2012-CLS
dataset for image classification ("Imagenet").

## Usage

This module implements the common signature for computing
[image feature vectors](../../../../../common_signatures/images.md#image-feature-vector).
It can be used like

```python
module = hub.Module("https://storage.googleapis.com/tfhub-test-modules/google/imagenet/mobilenet_v2_050_160/feature_vector/1.tar.gz")
height, width = hub.get_expected_image_size(module)
images = ...  # A batch of images with shape [batch_size, height, width, 3].
features = module(images)  # Features with shape [batch_size, num_features].
```

...or using the signature name `image_feature_vector`. The output for each image
in the batch is a feature vector of size `num_features` = 1280.

For this module, the size of the input image is fixed to
`height` x `width` = 160 x 160 pixels.
The input `images` are expected to have color values in the range [0,1],
following the
[common image input](../../../../../common_signatures/images.md#image-input)
conventions.


## Fine-tuning

Consumers of this module can [fine-tune](../../../../../fine_tuning.md) it.
This requires importing the graph version with tag set `{"train"}`
in order to operate batch normalization in training mode.

