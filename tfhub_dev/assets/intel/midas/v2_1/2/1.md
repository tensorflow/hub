# Module intel/midas/v2_1/2
Mobile convolutional neural network for monocular depth estimation from a single RGB image.

<!-- asset-path: https://github.com/intel-isl/MiDaS/releases/download/v2_1/model_hub_small.tar.gz -->
<!-- module-type: image-depth-estimation -->
<!-- network-architecture: MiDaS -->
<!-- dataset: DIML Indoor, MegaDepth, ReDWeb, WSVD, 3D Movies, TartanAir, HRWSI, ApolloScape, BlendedMVS, IRS -->
<!-- fine-tunable: false  -->
<!-- format: hub -->
<!-- license: MIT -->

## License
MIT License

## Qualitative Information

This MiDaS model is converted from the original version of the [MiDaS model](https://github.com/intel-isl/MiDaS). 
MiDaS was first introduced by
Rene Ranftl, Katrin Lasinger, David Hafner, Konrad Schindler, Vladlen Koltun in the paper:
[Towards Robust Monocular Depth Estimation: Mixing Datasets for Zero-shot Cross-dataset Transfer](https://arxiv.org/abs/1907.01341).

#### Requirements

```
pip install tensorflow tensorflow-hub opencv-python
```

* required nVidia GPU

#### Model Details
This module is based on a Pytorch version of MiDaS, which performs efficiently and with very high accuracy to compute depth from a single image.

* input: (uint8) RGB image with shape (3, 256, 256)
* output: (float32) inverse depth maps (1, 256, 256)

#### Example Use

```python
import cv2
import numpy as np
import urllib.request

import tensorflow as tf
import tensorflow_hub as hub

url, filename = ("https://github.com/intel-isl/MiDaS/releases/download/v2/dog.jpg", "dog.jpg")
urllib.request.urlretrieve(url, filename)

# the runtime initialization will not allocate all memory on the device to avoid out of GPU memory
gpus = tf.config.experimental.list_physical_devices('GPU')
for gpu in gpus:
    #tf.config.experimental.set_memory_growth(gpu, True)
    tf.config.experimental.set_virtual_device_configuration(gpu,
    [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=4000)])

# input
img = cv2.imread('dog.jpg')
img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) / 255.0

img_resized = tf.image.resize(img, [256,256], method='bicubic', preserve_aspect_ratio=False)
img_resized = tf.transpose(img_resized, [2, 0, 1])
img_input = img_resized.numpy()
reshape_img = img_input.reshape(1,3,256,256)
tensor = tf.convert_to_tensor(reshape_img, dtype=tf.float32)

# load model
module = hub.load("https://tfhub.dev/intel/midas/v2_1/2", tags=['serve'])
output = module.signatures['serving_default'](tensor)
prediction = output['default'].numpy()
prediction = prediction.reshape(256, 256)
             
# output file
prediction = cv2.resize(prediction, (img.shape[1], img.shape[0]), interpolation=cv2.INTER_CUBIC)
print(" Write image to: output.png")
depth_min = prediction.min()
depth_max = prediction.max()
img_out = (255 * (prediction - depth_min) / (depth_max - depth_min)).astype("uint8")
cv2.imwrite("output.png", img_out)

```


**Source:** https://github.com/intel-isl/MiDaS
