# Module google/&zwnj;inaturalist/&zwnj;inception_v3/&zwnj;feature_vector/1
Feature vectors of images with Inception V3 trained on the iNaturalist (iNat) 2017 dataset.

<!-- dataset: iNaturalist (iNat) 2017 -->
<!-- module-type: image-feature-vector -->
<!-- network-architecture: Inception V3 -->

**Module URL:** [https://tfhub.dev/google/inaturalist/inception_v3/feature_vector/1](https://tfhub.dev/google/inaturalist/inception_v3/feature_vector/1)

## Overview

This module uses the Inception V3 architecture, trained on the iNaturalist
dataset of plants and animals.

### Architecture

Inception V3 is a neural network architecture for image classification,
originally published by

  * Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens,
    Zbigniew Wojna: ["Rethinking the Inception Architecture for Computer
    Vision"](https://arxiv.org/abs/1512.00567), 2015.

This TF-Hub module uses the TF-Slim implementation of `inception_v3`.
The module contains a trained instance of the network, packaged to get
[feature vectors from images](https://www.tensorflow.org/hub/common_signatures/images.md#feature-vector).
The classification layer has been omitted.


### Training

The weights for this module were obtained by training on the iNaturalist
(iNat) 2017 dataset, after pre-training on ILSVRC-2012-CLS ("Imagenet").

The iNat2017 dataset consists of 579,184 training images and 95,986 validation
images from 5,089 species, taken from
[www.inaturalist.org](http://www.inaturalist.org).
Images were collected with different camera types, have varying image quality,
feature a large class imbalance, and have been verified by multiple
citizen scientists. The iNat2017 dataset was originally described in

  * Grant Van Horn, Oisin Mac Aodha, Yang Song, Yin Cui, Chen Sun,
    Alex Shepard, Hartwig Adam, Pietro Perona, Serge Belongie:
    ["The iNaturalist Species Classification and Detection
    Dataset"](https://arxiv.org/abs/1707.06642), CVPR 2018.

This model was trained for the study reported in

  * Yin Cui, Yang Song, Chen Sun, Andrew Howard, Serge Belongie:
    ["Large Scale Fine-Grained Categorization and Domain-Specific Transfer
    Learning"](https://arxiv.org/abs/1806.06193), CVPR 2018.

Training was done on a Cloud TPU, with "Inception-style" data
augmentation and preprocessing as for Imagenet, using the RMSProp optimizer
with epsilon = 1.0, momentum of 0.9, and a batch size of 32.
The network was trained from a checkpoint pretrained on ILSVRC-2012-CLS
("Imagenet"). The initial learning rate was set to 0.0045, with exponential
decay of 0.94 after every 4 epochs.


## Usage

This module implements the common signature for computing
[image feature vectors](https://www.tensorflow.org/hub/common_signatures/images#feature-vector).
It can be used like

```python
module = hub.Module("https://tfhub.dev/google/inaturalist/inception_v3/feature_vector/1")
height, width = hub.get_expected_image_size(module)
images = ...  # A batch of images with shape [batch_size, height, width, 3].
features = module(images)  # Features with shape [batch_size, num_features].
```

...or using the signature name `image_feature_vector`. The output for each image
in the batch is a feature vector of size `num_features` = 2048.

For this module, the size of the input image is fixed to
`height` x `width` = 299 x 299 pixels.
The input `images` are expected to have color values in the range [0,1],
following the
[common image input](https://www.tensorflow.org/hub/common_signatures/images#input)
conventions.


## Fine-tuning

Consumers of this module can [fine-tune](https://www.tensorflow.org/hub/fine_tuning) it.
This requires importing the graph version with tag set `{"train"}`
in order to operate batch normalization in training mode.
