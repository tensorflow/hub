{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "RUymE2l9GZfO"
      },
      "source": [
        "**Copyright 2019 The TensorFlow Hub Authors.**\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "cellView": "code",
        "colab": {},
        "colab_type": "code",
        "id": "JMyTNwSJGGWg"
      },
      "outputs": [],
      "source": [
        "# Copyright 2019 The TensorFlow Hub Authors. All Rights Reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "# =============================================================================="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "co7MV6sX7Xto"
      },
      "source": [
        "# Cross-Lingual Similarity and Semantic Search Engine with Multilingual Universal Sentence Encoder\n",
        "\n",
        "\n",
        "\u003ctable align=\"left\"\u003e\u003ctd\u003e\n",
        "  \u003ca target=\"_blank\"  href=\"https://colab.research.google.com/github/tensorflow/hub/blob/master/examples/colab/cross_lingual_similarity_with_tf_hub_multilingual_universal_encoder.ipynb\"\u003e\n",
        "    \u003cimg src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" /\u003eRun in Google Colab\n",
        "  \u003c/a\u003e\n",
        "\u003c/td\u003e\u003ctd\u003e\n",
        "  \u003ca target=\"_blank\"  href=\"https://github.com/tensorflow/hub/blob/master/examples/colab/cross_lingual_similarity_with_tf_hub_multilingual_universal_encoder.ipynb\"\u003e\n",
        "    \u003cimg width=32px src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" /\u003eView source on GitHub\u003c/a\u003e\n",
        "\u003c/td\u003e\u003c/table\u003e\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "eAVQGidpL8v5"
      },
      "source": [
        "This notebook illustrates how to access the Multilingual Universal Sentence Encoder module and use it for sentence similarity across multiple languages. This module is an extension of the [original Universal Encoder module](https://tfhub.dev/google/universal-sentence-encoder/2).\n",
        "\n",
        "The notebook is divided as follows:\n",
        "\n",
        "*   The first section shows a visualization of sentences between pair of languages. This is a more academic exercise.\n",
        "*   In the second section, we show how to build a semantic search engine from a sample of a Wikipedia corpus in multiple languages."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "pOTzp8O36CyQ"
      },
      "source": [
        "# Getting Started\n",
        "\n",
        "This section sets up the environment for access to the Multilingual Universal Sentence Encoder Module and also prepares a set of English sentences and their translations. In the following sections, the multilingual module will be used to compute similarity *across languages*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "63Pd3nJnTl-i"
      },
      "source": [
        "**IMPORTANT**Note: Pleaseelect \"**Python 3**\" _and_ \"**GPU**\" in the ***Runtime-\u003eChange Runtime type*** dropdown menu above _before_ running this notebook for faster execution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "cellView": "form",
        "colab": {},
        "colab_type": "code",
        "id": "lVjNK8shFKOC"
      },
      "outputs": [],
      "source": [
        "#@title Setup Environment\n",
        "#latest Tensorflow that supports sentencepiece is 1.13.1\n",
        "!pip uninstall --quiet --yes tensorflow\n",
        "!pip install --quiet tensorflow-gpu==1.13.1\n",
        "!pip install --quiet tensorflow-hub\n",
        "!pip install --quiet seaborn\n",
        "!pip install --quiet tf-sentencepiece\n",
        "!pip install --quiet simpleneighbors\n",
        "!pip install --quiet tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "cellView": "form",
        "colab": {},
        "colab_type": "code",
        "id": "MSeY-MUQo2Ha"
      },
      "outputs": [],
      "source": [
        "#@title Setup common imports and functions\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tf_sentencepiece  # Not used directly but needed to import TF ops.\n",
        "\n",
        "from simpleneighbors import SimpleNeighbors\n",
        "from tqdm import tqdm\n",
        "from tqdm import trange\n",
        "\n",
        "def visualize_similarity(embeddings_1, embeddings_2, labels_1, labels_2, plot_title):\n",
        "  corr = np.inner(embeddings_1, embeddings_2)\n",
        "  chart = sns.heatmap(corr,\n",
        "                      xticklabels=labels_1,\n",
        "                      yticklabels=labels_2,\n",
        "                      vmin=0,\n",
        "                      vmax=1,\n",
        "                      cmap='YlOrRd')\n",
        "  chart.set_yticklabels(chart.get_yticklabels(), rotation=0)\n",
        "  chart.set_title(plot_title)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gk2IRjZFGDsK"
      },
      "source": [
        "This is an additional boilerplate code where where we import the pre-trained ML model we will use to encode text throughout this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "mkmF3w8WGLcM"
      },
      "outputs": [],
      "source": [
        "# The 16-language multilingual module is the default but feel free\n",
        "# to pick others from the list and compare the results.\n",
        "module_url = 'https://tfhub.dev/google/universal-sentence-encoder-multilingual/1'  #@param ['https://tfhub.dev/google/universal-sentence-encoder-multilingual/1', 'https://tfhub.dev/google/universal-sentence-encoder-multilingual-large/1', 'https://tfhub.dev/google/universal-sentence-encoder-xling-many/1']\n",
        "\n",
        "# Set up graph.\n",
        "g = tf.Graph()\n",
        "with g.as_default():\n",
        "  text_input = tf.placeholder(dtype=tf.string, shape=[None])\n",
        "  multiling_embed = hub.Module(module_url)\n",
        "  embedded_text = multiling_embed(text_input)\n",
        "  init_op = tf.group([tf.global_variables_initializer(), tf.tables_initializer()])\n",
        "g.finalize()\n",
        "\n",
        "# Initialize session.\n",
        "session = tf.Session(graph=g)\n",
        "session.run(init_op)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jhLPq6AROyFk"
      },
      "source": [
        "# Visualize Text Similarity Between Languages\n",
        "With the sentence embeddings now in hand, we can visualize semantic similarity across different languages."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8xdAogbxJDTD"
      },
      "source": [
        "## Computing Text Embeddings\n",
        "\n",
        "We first define a set of sentences translated to various languages in parallel. Then, we precompute the embeddings for all of our sentences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "Q8F4LNGFqOiq"
      },
      "outputs": [],
      "source": [
        "# Some texts of different lengths in different languages.\n",
        "english_sentences = ['dog', 'Puppies are nice.', 'I enjoy taking long walks along the beach with my dog.']\n",
        "spanish_sentences = ['perro', 'Los cachorros son agradables.', 'Disfruto de dar largos paseos por la playa con mi perro.']\n",
        "german_sentences = ['Hund', 'Welpen sind nett.', 'Ich genieße lange Spaziergänge am Strand entlang mit meinem Hund.']\n",
        "french_sentences = ['chien', 'Les chiots sont gentils.', 'J\\'aime faire de longues promenades sur la plage avec mon chien.']\n",
        "italian_sentences = ['cane', 'I cuccioli sono carini.', 'Mi piace fare lunghe passeggiate lungo la spiaggia con il mio cane.']\n",
        "chinese_sentences = ['狗', '小狗很好。', '我喜欢和我的狗一起沿着海滩散步。']\n",
        "korean_sentences = ['개', '강아지가 좋다.', '나는 나의 산책을 해변을 따라 길게 산책하는 것을 즐긴다.']\n",
        "japanese_sentences = ['犬', '子犬はいいです', '私は犬と一緒にビーチを散歩するのが好きです']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "weXZqLtTJY9b"
      },
      "outputs": [],
      "source": [
        "# Compute embeddings.\n",
        "en_result = session.run(embedded_text, feed_dict={text_input: english_sentences})\n",
        "es_result = session.run(embedded_text, feed_dict={text_input: spanish_sentences})\n",
        "de_result = session.run(embedded_text, feed_dict={text_input: german_sentences})\n",
        "fr_result = session.run(embedded_text, feed_dict={text_input: french_sentences})\n",
        "it_result = session.run(embedded_text, feed_dict={text_input: italian_sentences})\n",
        "zh_result = session.run(embedded_text, feed_dict={text_input: chinese_sentences})\n",
        "ko_result = session.run(embedded_text, feed_dict={text_input: korean_sentences})\n",
        "ja_result = session.run(embedded_text, feed_dict={text_input: japanese_sentences})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_3zGWuF-GhUm"
      },
      "source": [
        "## Visualizing Similarity\n",
        "\n",
        "With text embeddings in hand, we can take their dot-product to visualize how similar sentences are between languages. A darker color indicates the embeddings are semantically similar."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "BJkL6Az0QXNN"
      },
      "source": [
        "### English-Spanish Similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "CH_BXVGhQ0GL"
      },
      "outputs": [],
      "source": [
        "visualize_similarity(en_result, es_result, english_sentences, spanish_sentences, 'English-Spanish Similarity')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "imn28LCiQO7d"
      },
      "source": [
        "### English-Italian Similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "X9uD3DirPIGd"
      },
      "outputs": [],
      "source": [
        "visualize_similarity(en_result, it_result, english_sentences, italian_sentences, 'English-Italian Similarity')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "m6ySvEGbQaTM"
      },
      "source": [
        "### Italian-Spanish Similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "irfwIeitQ7V6"
      },
      "outputs": [],
      "source": [
        "visualize_similarity(it_result, es_result, italian_sentences, spanish_sentences, 'Italian-Spanish Similarity')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rRabHHQYQfLr"
      },
      "source": [
        "### And more...\n",
        "\n",
        "The above examples can be extended to any language pair from **English, Arabic, Chinese, Dutch, French, German, Italian, Japanese, Korean, Polish, Portuguese, Russian, Spanish, Thai and Turkish**. Happy coding!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "mxAFAJI9xsAU"
      },
      "source": [
        "# Creating a Multilingual Semantic-Similarity Search Engine\n",
        "\n",
        "Whereas in the previous example we visualized a handful of sentences, in this section we will build a semantic-search index of about 200,000 sentences from a Wikipedia Corpus. About half will be in English and the other half in Spanish to demonstrate the multilingual capabilities of the Universal Sentence Encoder.\n",
        "\n",
        "## Download Data to Index\n",
        "First, we will download news sentences in multiples languages from the [News Commentary Corpus](http://opus.nlpl.eu/News-Commentary-v11.php) [1].  Without loss of generality, this approach should also work for indexing the rest of the supported languages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "587I9ye6yXEU"
      },
      "outputs": [],
      "source": [
        "corpus_metadata = [\n",
        "    ('ar', 'ar-en.txt.zip', 'News-Commentary.ar-en.ar', 'Arabic'),\n",
        "    ('zh', 'en-zh.txt.zip', 'News-Commentary.en-zh.zh', 'Chinese'),\n",
        "    ('en', 'en-es.txt.zip', 'News-Commentary.en-es.en', 'English'),\n",
        "    ('ru', 'en-ru.txt.zip', 'News-Commentary.en-ru.ru', 'Russian'),\n",
        "    ('es', 'en-es.txt.zip', 'News-Commentary.en-es.es', 'Spanish'),\n",
        "]\n",
        "\n",
        "language_to_sentences = {}\n",
        "language_to_news_path = {}\n",
        "for language_code, zip_file, news_file, language_name in corpus_metadata:\n",
        "  zip_path = tf.keras.utils.get_file(\n",
        "      fname=zip_file,\n",
        "      origin='http://opus.nlpl.eu/download.php?f=News-Commentary/v11/moses/' + zip_file,\n",
        "      extract=True)\n",
        "  news_path = os.path.join(os.path.dirname(zip_path), news_file)\n",
        "  language_to_sentences[language_code] = pd.read_csv(news_path, sep='\\t', header=None)[0]\n",
        "  language_to_news_path[language_code] = news_path\n",
        "\n",
        "  print('{:,} {} sentences'.format(len(language_to_sentences[language_code]), language_name))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "m3DIT9uT7Z34"
      },
      "source": [
        "## Using a pre-trained model to transform sentences into vectors\n",
        "\n",
        "We comute embeddings in _batches_ so that they fit in the GPU's RAM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "yRoRT5qCEIYy"
      },
      "outputs": [],
      "source": [
        "# Takes about 3 minutes\n",
        "\n",
        "batch_size = 2048\n",
        "language_to_embeddings = {}\n",
        "for language_code, zip_file, news_file, language_name in corpus_metadata:\n",
        "  print('\\nComputing {} embeddings'.format(language_name))\n",
        "  with tqdm(total=len(language_to_sentences[language_code])) as pbar:\n",
        "    for batch in pd.read_csv(language_to_news_path[language_code], sep='\\t',header=None, chunksize=batch_size):\n",
        "      language_to_embeddings.setdefault(language_code, []).extend(session.run(embedded_text, feed_dict={text_input: batch[0]}))\n",
        "      pbar.update(len(batch))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "oeBqoE8e-scg"
      },
      "source": [
        "## Building an index of semantic vectors\n",
        "\n",
        "We use the [SimpleNeighbors](https://pypi.org/project/simpleneighbors/) library---which is a wrapper for [Annoy](https://github.com/spotify/annoy) library---to efficiently look up results from the corpus."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "lv_SOduAF1oi"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "# Takes about 8 minutes\n",
        "\n",
        "num_index_trees = 40\n",
        "language_name_to_index = {}\n",
        "embedding_dimensions = len(list(language_to_embeddings.values())[0][0])\n",
        "for language_code, zip_file, news_file, language_name in corpus_metadata:\n",
        "  print('\\nAdding {} embeddings to index'.format(language_name))\n",
        "  index = SimpleNeighbors(embedding_dimensions, metric='dot')\n",
        "\n",
        "  for i in trange(len(language_to_sentences[language_code])):\n",
        "    index.add_one(language_to_sentences[language_code][i], language_to_embeddings[language_code][i])\n",
        "\n",
        "  print('Building {} index with {} trees...'.format(language_name, num_index_trees))\n",
        "  index.build(n=num_index_trees)\n",
        "  language_name_to_index[language_name] = index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "0aqGwIuLGrtu"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "# Takes about 13 minutes\n",
        "\n",
        "num_index_trees = 60\n",
        "print('Computing mixed-language index')\n",
        "combined_index = SimpleNeighbors(embedding_dimensions, metric='dot')\n",
        "for language_code, zip_file, news_file, language_name in corpus_metadata:\n",
        "  print('Adding {} embeddings to mixed-language index'.format(language_name))\n",
        "  for i in trange(len(language_to_sentences[language_code])):\n",
        "    annotated_sentence = '({}) {}'.format(language_name, language_to_sentences[language_code][i])\n",
        "    combined_index.add_one(annotated_sentence, language_to_embeddings[language_code][i])\n",
        "\n",
        "print('Building mixed-language index with {} trees...'.format(num_index_trees))\n",
        "combined_index.build(n=num_index_trees)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kg9cw0S2_ntQ"
      },
      "source": [
        "## Verify that the semantic-similarity search engine works\n",
        "\n",
        "In this section will demonstrate both:\n",
        "\n",
        "1.   Semantic-search capabilities: retrieving sentences from the corpus that are semantically similar to the given query.\n",
        "2.   Multilingual capabilities: doing so in multiple languages when they query language and index language match\n",
        "3.   Cross-lingual capabilities: issuing queries in a distinct language than the indexed corpus\n",
        "4.   Mixed-language corpus: all of the above on a single index containing entries from all languages\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Dxu66S8wJIG9"
      },
      "source": [
        "### Semantic-search crosss-lingual capabilities\n",
        "\n",
        "In this section we show how to retrieve sentences related to a set of sample English sentences. Things to try:\n",
        "\n",
        "*   Try a few different sample sentences\n",
        "*   Try changing the number of returned results (they are returned in order of similarity)\n",
        "*   Try cross-lingual capabilities by returning results in different languages (might want to use [Google Translate](http://translate.google.com) on some results to your native language for sanity check)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "cellView": "form",
        "colab": {},
        "colab_type": "code",
        "id": "_EFSd65B_mq8"
      },
      "outputs": [],
      "source": [
        "sample_query = 'The stock market fell four points.'  #@param [\"Global warming\", \"Researchers made a surprising new discovery last week.\", \"The stock market fell four points.\", \"Lawmakers will vote on the proposal tomorrow.\"] {allow-input: true}\n",
        "index_language = 'English'  #@param [\"Arabic\", \"Chinese\", \"English\", \"French\", \"German\", \"Russian\", \"Spanish\"]\n",
        "num_results = 10  #@param {type:\"slider\", min:0, max:100, step:10}\n",
        "\n",
        "query_embedding = session.run(embedded_text, feed_dict={text_input: [sample_query]})[0]\n",
        "\n",
        "search_results = language_name_to_index[index_language].nearest(query_embedding, n=num_results)\n",
        "\n",
        "print('{} sentences similar to: \"{}\"\\n'.format(index_language, sample_query))\n",
        "search_results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Ybgj9o7hKDZV"
      },
      "source": [
        "### Mixed-corpus capabilities\n",
        "\n",
        "We will now issue a query in English, but the results will come from the any of the indexed languages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "cellView": "form",
        "colab": {},
        "colab_type": "code",
        "id": "MJeTzuj0KU41"
      },
      "outputs": [],
      "source": [
        "sample_query = 'The stock market fell four points.'  #@param [\"Global warming\", \"Researchers made a surprising new discovery last week.\", \"The stock market fell four points.\", \"Lawmakers will vote on the proposal tomorrow.\"] {allow-input: true}\n",
        "num_results = 40  #@param {type:\"slider\", min:0, max:100, step:10}\n",
        "query_embedding = session.run(embedded_text, feed_dict={text_input: [sample_query]})[0]\n",
        "\n",
        "search_results = combined_index.nearest(query_embedding, n=num_results)\n",
        "\n",
        "print('{} sentences similar to: \"{}\"\\n'.format(index_language, sample_query))\n",
        "search_results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "tqIRtHIL2jAw"
      },
      "source": [
        "Try your own queries:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "cellView": "form",
        "colab": {},
        "colab_type": "code",
        "id": "ZK5ID6XF2n8k"
      },
      "outputs": [],
      "source": [
        "query = 'The stock market fell four points.'  #@param {type:\"string\"}\n",
        "num_results = 30  #@param {type:\"slider\", min:0, max:100, step:10}\n",
        "query_embedding = session.run(embedded_text, feed_dict={text_input: [query]})[0]\n",
        "\n",
        "search_results = combined_index.nearest(query_embedding, n=num_results)\n",
        "\n",
        "print('{} sentences similar to: \"{}\"\\n'.format(index_language, query))\n",
        "search_results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "IPPwnhUNdOfc"
      },
      "source": [
        "# Further topics\n",
        "\n",
        "## Multilingual\n",
        "\n",
        "Finally, we encourage you to tried queries in any of the supported languages: **English, Arabic, Chinese, Dutch, French, German, Italian, Japanese, Korean, Polish, Portuguese, Russian, Spanish, Thai and Turkish**.\n",
        "\n",
        "Also, even though we only indexed in a subset of the languages, you can also index content in any of the supported languages.\n",
        "\n",
        "\n",
        "## Model variations\n",
        "\n",
        "We offer variations of the Universal Encoder models optimized for various things like memory, latency and/or quality. Please feel free to experiment with them to find a suitable one.\n",
        "\n",
        "## Nearest neighbor libraries\n",
        "\n",
        "We used a Annoy to efficiently look up nearest neighbors. See the [tradeoffs section](https://github.com/spotify/annoy/blob/master/README.rst#tradeoffs) to read about the number of trees (memory-dependent) and number of items to search (latency-dependent)---SimpleNeighbors only allows to control the number of trees, but refactoring the code to use Annoy directly should be simple, we just wanted to keep this code as simple as possible for the general user.\n",
        "\n",
        "If Annoy does not scale for your application, please also check out [FAISS](https://github.com/facebookresearch/faiss)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5yj9VcfnbS-q"
      },
      "source": [
        "*All the best building your multilingual semantic applications!*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "X4oOfvSOKnjS"
      },
      "source": [
        "[1] J. Tiedemann, 2012, [Parallel Data, Tools and Interfaces in OPUS](http://www.lrec-conf.org/proceedings/lrec2012/pdf/463_Paper.pdf). In Proceedings of the 8th International Conference on Language Resources and Evaluation (LREC 2012)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Cross-Lingual Similarity and Semantic Search Engine with TF-Hub Multilingual Universal Encoder",
      "provenance": [],
      "version": "0.3.2"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
